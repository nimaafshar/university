{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 230 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Downloading soupsieve-2.1-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC,abstractmethod\n",
    "import bs4\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "class Crawler(ABC):\n",
    "    \n",
    "    def __init__(self,data_dir,base_url,saving_interval=100):\n",
    "        self.crawling_paths = []\n",
    "        self.base_url = base_url\n",
    "        self.data = []\n",
    "        self.data_dir = data_dir\n",
    "        self.crawling_paths_error = []\n",
    "        self.paths_error = []\n",
    "        self.saving_interval = saving_interval\n",
    "        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_childs(self,items_count=10):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def crawl_data(self,path):\n",
    "        payload={}\n",
    "        return payload\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_page_childs(self,path):\n",
    "        pass\n",
    "    \n",
    "    def error_occured(self):\n",
    "        return len(self.crawling_paths_error) > 0\n",
    "    \n",
    "    def save(self):\n",
    "        with open(self.data_dir+'/errors.json','w') as errors_file,open(self.data_dir+'/data.json','w') as data_file:\n",
    "            json.dump({\n",
    "                'crawling_paths_error':self.crawling_paths_error,\n",
    "                'paths_error':self.paths_error\n",
    "            },errors_file,indent=2)\n",
    "            json.dump(self.data,data_file,indent=2)\n",
    "    \n",
    "    def run(self,items_count=10):\n",
    "        self.get_childs(items_count)\n",
    "        for i,path in enumerate(self.crawling_paths):\n",
    "            if i%self.saving_interval==0:\n",
    "                self.save()\n",
    "                print(f'{i} datapoints saved')\n",
    "            self.data.append(self.crawl_data(path))\n",
    "        self.save()\n",
    "        print(f'{i+1} datapoints saved')\n",
    "    \n",
    "    def run_failed_attempts(self):\n",
    "        cpe = self.crawling_paths_error.copy()\n",
    "        self.crawling_paths = self.paths_error.copy()\n",
    "        self.crawling_paths_error = []\n",
    "        self.paths_error = []\n",
    "        for item in cpe:\n",
    "            self.get_page_childs(item['URL'])\n",
    "        for i,path in enumerate(self.crawling_paths):\n",
    "            if i%self.saving_interval==0:\n",
    "                self.save()\n",
    "                print(f'{i} datapoints saved')\n",
    "            self.data.append(self.crawl_data(path))\n",
    "        self.save()\n",
    "        print(f'{i+1} datapoints saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouzzCrawler(Crawler):\n",
    "    items_per_page = 36\n",
    "    \n",
    "    def get_childs(self,items_count=10):\n",
    "        remaining_items = items_count\n",
    "        while remaining_items > 0:\n",
    "            page = (items_count-remaining_items)//self.items_per_page\n",
    "            URL = f'{self.base_url}/p/{page}?oq=' if page > 0 else self.base_url\n",
    "#             print(URL)\n",
    "            try:\n",
    "                response = requests.get(URL)\n",
    "            except error as e:\n",
    "                self.crawling_paths_error.append({\n",
    "                    'URL': URL,\n",
    "                    'error': e\n",
    "                })\n",
    "            if response.status_code !=200:\n",
    "                self.crawling_paths_error.append({\n",
    "                    'URL': URL,\n",
    "                    'status_code': status_code\n",
    "                })\n",
    "            soup = bs4.BeautifulSoup(response.content,'html.parser')\n",
    "            a_tags = soup.findAll('a', class_='hz-product-card__link',href=True)\n",
    "#             print(len(a_tags))\n",
    "            for a_tag in a_tags:\n",
    "                if a_tag.get('href') is None:\n",
    "                    pass\n",
    "                self.crawling_paths.append(a_tag.get('href'))\n",
    "                remaining_items -=1\n",
    "                if remaining_items <=0:\n",
    "                    break\n",
    "            \n",
    "    def get_page_childs(self,path):\n",
    "            try:\n",
    "                response = requests.get(URL)\n",
    "            except error as e:\n",
    "                self.crawling_paths_error.append({\n",
    "                    'URL': URL,\n",
    "                    'error': e\n",
    "                })\n",
    "            if response.status_code !=200:\n",
    "                self.crawling_paths_error.append({\n",
    "                    'URL': URL,\n",
    "                    'status_code': status_code\n",
    "                })\n",
    "            soup = bs4.BeautifulSoup(response.content,'html.parser')\n",
    "            a_tags = soup.findAll('a', class_='hz-product-card__link',href=True)\n",
    "            for a_tag in a_tags:\n",
    "                if a_tag.get('href') is None:\n",
    "                    pass\n",
    "                self.crawling_paths.append(a_tag.get('href'))\n",
    "    \n",
    "    def crawl_data(self,path):\n",
    "        try:\n",
    "            response = requests.get(path)\n",
    "        except error as e:\n",
    "            self.paths_error.append({\n",
    "                'URL': path,\n",
    "                'error': e\n",
    "            })\n",
    "            \n",
    "        if response.status_code !=200:\n",
    "            self.paths_error.append({\n",
    "                'URL': path,\n",
    "                'status_code': status_code\n",
    "            })\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(response.content,'html.parser')\n",
    "        description_tags = [tag.get_text() for tag in soup.find_all('li', class_='product-keywords__word')]\n",
    "        title = soup.find('span',class_='view-product-title').get_text()\n",
    "        thumb_divs = soup.find_all('div',class_='alt-images__thumb')\n",
    "        thumbnails = [thumb_div.find('img').get('src') for thumb_div in thumb_divs]\n",
    "        sample_img_url = soup.find('img',class_='view-product-image-print').get('src')\n",
    "        replacement = sample_img_url[sample_img_url.find('_'):]\n",
    "        images = []\n",
    "        for thumbnail in thumbnails:\n",
    "            image_path = thumbnail.replace('fimgs','simgs',1)\n",
    "            image_path = re.sub(r'_.*',replacement,image_path)\n",
    "            images.append(image_path)\n",
    "        return {\n",
    "            'title':title,\n",
    "            'This Product Has Been Described As': description_tags,\n",
    "            'thumbnails': thumbnails,\n",
    "            'images': images\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 datapoints saved\n",
      "5 datapoints saved\n",
      "10 datapoints saved\n"
     ]
    }
   ],
   "source": [
    "hc = HouzzCrawler(data_dir='./data/bed',base_url='https://www.houzz.com/products/beds-and-headboards',saving_interval=5)\n",
    "hc.run(items_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 datapoints saved\n",
      "10 datapoints saved\n",
      "20 datapoints saved\n"
     ]
    }
   ],
   "source": [
    "hc = HouzzCrawler(data_dir='./data/chair',base_url='https://www.houzz.com/products/chairs',saving_interval=10)\n",
    "hc.run(items_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 datapoints saved\n",
      "15 datapoints saved\n"
     ]
    }
   ],
   "source": [
    "hc = HouzzCrawler(data_dir='./data/sofa',base_url='https://www.houzz.com/products/sofas-and-sectionals',saving_interval=20)\n",
    "hc.run(items_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 datapoints saved\n",
      "10 datapoints saved\n",
      "15 datapoints saved\n"
     ]
    }
   ],
   "source": [
    "hc = HouzzCrawler(data_dir='./data/desk',base_url='https://www.houzz.com/products/desks',saving_interval=10)\n",
    "hc.run(items_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
